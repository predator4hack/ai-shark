"""
Evaluation Agent for AI-Shark

This agent evaluates the quality of questions generated by the QuestionnaireAgent.
It uses a Strategy pattern to select an evaluation method based on the availability
of a founder checklist.
"""

import os
from typing import Optional, Dict, Any
from langchain_core.prompts import PromptTemplate

from src.agents.base_agent import BaseStructuredAgent
from src.models.evaluation_models import EvaluationResult
from src.strategies.evaluation_strategy import (
    EvaluationStrategy,
    ChecklistEvaluationStrategy,
    ExpertEvaluationStrategy,
)
from src.utils.prompt_manager import PromptManager

class EvaluationAgent(BaseStructuredAgent):
    """
    An agent that evaluates generated questionnaires. It dynamically selects
    an evaluation strategy (checklist-based or expert-based) at runtime.
    """

    def __init__(self, strategy: EvaluationStrategy, **kwargs):
        """
        Initializes the EvaluationAgent with a specific evaluation strategy.

        Args:
            strategy: The evaluation strategy to use (e.g., ChecklistEvaluationStrategy).
            **kwargs: Additional arguments for the BaseStructuredAgent.
        """
        super().__init__(
            agent_name="EvaluationAgent",
            output_model=EvaluationResult,
            **kwargs
        )
        self.strategy = strategy

    def get_system_prompt(self) -> str:
        """
        Returns a consistent system prompt for the evaluation agent.
        """
        return "You are a venture capital expert specializing in startup due diligence."

    def get_analysis_prompt_template(self, **kwargs) -> PromptTemplate:
        """
        Delegates prompt template creation to the current strategy.
        """
        return self.strategy.get_analysis_prompt_template(**kwargs)

    def analyze(self, generated_questions: str, founder_checklist: Optional[str] = None) -> EvaluationResult:
        """
        Performs the evaluation by running the structured analysis pipeline.

        Args:
            generated_questions: The text of the questions to be evaluated.
            founder_checklist: The text of the founder checklist, if available.

        Returns:
            An EvaluationResult Pydantic model with the structured evaluation.
        """
        input_vars = {
            "generated_questions": generated_questions,
            "founder_checklist": founder_checklist or "",
        }
        
        # The base class's analyze method will call get_analysis_prompt_template,
        # format the prompt, execute the LLM call, and parse the output.
        # We are overriding the default signature to match our specific inputs.
        return super().analyze(document=None, **input_vars) # Pass None for document as it's not used

    def _prepare_analysis_input(self, document: Optional[Any], **kwargs) -> Dict[str, Any]:
        """
        Overrides the base method to prepare inputs from kwargs directly,
        as this agent doesn't operate on a standard StartupDocument.
        """
        return kwargs

    def _validate_document(self, document: Optional[Any]) -> bool:
        """
        Overrides and bypasses document validation as it's not applicable.
        """
        return True


def create_evaluation_agent(founder_checklist_path: Optional[str] = None) -> EvaluationAgent:
    """
    Factory function to create and configure the EvaluationAgent.

    It checks for the existence of the founder checklist and selects the
    appropriate strategy.

    Args:
        founder_checklist_path: The file path to the founder checklist.

    Returns:
        A fully configured EvaluationAgent instance.
    """
    prompt_manager = PromptManager()
    
    if founder_checklist_path and os.path.exists(founder_checklist_path):
        print("Found founder checklist. Using ChecklistEvaluationStrategy.")
        strategy = ChecklistEvaluationStrategy(prompt_manager)
    else:
        print("Founder checklist not found. Using ExpertEvaluationStrategy.")
        strategy = ExpertEvaluationStrategy(prompt_manager)
        
    return EvaluationAgent(strategy=strategy)